#!/usr/bin/env python3.9

import os
import re
import csv
import json
import glob
import sys
import argparse
import yaml
import subprocess

# Python scripts process promethus JSON raw metrics

def sys_exit(str):
    print(f"{str}")
    sys.exit(1)

def is_dir(dir_path):
    if not os.path.isdir(dir_path):
        print(f"The directory '{dir_path}' does not exits")
        return False
    return True

def file_is_readable(file_path):
    if not os.access(file_path, os.R_OK):
        print(f"The file at '{file_path}' is not readable")
        return False
    return True

def obj_exist(obj):
    return obj is not None

def check_meta_data(json_obj):
    status = json_obj.get("status")
    if obj_exist(status) and status == "success":
        print("Promethus query status returned success")
    else:
        sys_exit("Promethus query status returned non-success or status value is not present")
    data = json_obj.get("data")
    res_type = data.get("resultType")
    res = data.get("result")
    empty_metric = 0
    if obj_exist(data) and obj_exist(res_type) and obj_exist(res) and len(res) >= 1:
        for item in res:
            if len(item['metric']) == 0:
                empty_metric += 1
        print(f"total of {len(res)} entries of data found, {empty_metric} entires without metric name, resultType: {res_type}")
    else:
        sys_exit("No data found, please check your json file")

# read the path of json files with given directory
def read_json_files(dir_path):
    if is_dir(dir_path):
        return glob.glob(f"{dir_path}/*.json")

def read_csv_files(dir_path):
    if bool(re.match(r"^~", dir_path)):
        print(f"~ symbol found")
        dir_path = os.path.expanduser(dir_path)
    if is_dir(dir_path):
        print(f"globing path: {dir_path}")
        glob_obj = glob.glob(f"{dir_path}/*.csv")
        if len(glob_obj) < 1:
            sys_exit(f"no csv files found at {dir_path}")
        else:
            print(f"{len(glob_obj)} csv files found in total")
            return glob.glob(f"{dir_path}/*.csv")
    else:
        sys_exit(f"{dir_path} is not recognized as a directory")

def validate_csv_file_group(file_glob_obj):
    first_path = list(file_glob_obj)[0]
    file_name = re.search(r'[^/]+$', first_path)[0]
    file_attr = file_name.split("#")
    if len(file_attr) != 3:
        sys_exit("{first_path} does not look right, expecting 3 parts: namespace#iteration#metric_name, please double check")
    ns = file_attr[0]
    iterations = file_attr[1]
    metric_name = file_attr[2]
    directory = re.search(r'^(.+/)', first_path)[1]
    for index, path in enumerate(file_glob_obj):
        attr = re.search(r'[^/]+$', path)[0].split("#")
        if attr[0] != ns:
            sys_exit("{path} seems to belong to a different namespace, please double check")
        if index !=0:
            if attr[1] == iterations:
                sys_exit("{path} seems to have duplicated iteration entries, please double check")
        if attr[2] != metric_name:
            sys_exit("{path} seems to have different metric names, please double check")
    print("all csv files checks are done, ready to go")

def read_json_file(file_path):
    try:
        with open(file_path, "r") as file:
            return json.load(file)
    except FileNotFoundError:
        print(f"Error: file '{file_path}' does not exists")
    except IOError:
        print(f"Error: could not open file '{file_path}'")

def read_csv_file(file_path):
    try:
        return csv.reader(open(file_path, "r"))
    except FileNotFoundError:
        print(f"Error: file '{file_path}' does not exists")
    except IOError:
        print(f"Error: could not open file '{file_path}'")

# row expects an array
def csv_write_row(path, row):
    try:
        with open(path, mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(row)
    except Exception as e:
        print(f"Error: '{e}' occured while writing data into '{path}'")

"""
Assume promethus metric maintains a consistent structure as follows:
{ "status": "success",
  "data": {"resultType": "xx",
           "result": [
                      {"metric": {meta_data}, "values": [[time_stamp,value]...]},
                      {"metric": {meta_data}, "values": [[time_stamp,value]...]}
                     ]
          }
}
"""
# process json files from the entire directory
def json_dir_to_csv(dir_path):
    file_paths = read_json_files(dir_path)
    if len(file_paths) == 0:
        print("No json file is found")
        return
    for file_path in file_paths:
        json_obj = read_json_file(file_path)
        check_meta_data(json_obj)
        result = json_obj['data']["result"]
        headers = []
        values  = []
        for item in result:
            headers.append(item['metric']['pod'] + "-" + re.sub(r'^.*/([^/]+)\.\w+$', r'\1', file_path))
            values.append([value[1] for value in item['values']])
        # use zip to transpose lists to do column by column write
        csv_file_path=re.sub(r"\.json$", ".csv", file_path)
        csv_write_row(csv_file_path, headers)
        for row in zip(*values):
            csv_write_row(csv_file_path, row)
        print(f"Finished writting data into csv file at location: '{csv_file_path}'")

# structure a raw json object 
def process_raw_json_obj(json_obj):
    res = {}
    result = json_obj['data']["result"]
    for item in result:
        res[list(item['metric'].keys())[0]] = [float(value[1]) for value in item['values']]
    return res

def json_to_csv(file_path):
    if file_is_readable(file_path):
        json_obj = read_json_file(file_path)
    check_meta_data(json_obj)
    result = json_obj['data']["result"]
    csv_file_path=re.sub(r"\.json$", ".csv", file_path)
    item = result[0]
    for item in result:
        if len(item['metric']) == 0:
             continue
        values = []
        container_name = '-'.join([ item['metric'][key] for key in metric_names])
        print(item)
        values = [float(value[1]) for value in item['values']]
        non_zero_values = [value for value in values if value != float(0)]
        if len(non_zero_values) != len(values) or len(non_zero_values) == 0:
            print(f"found zero value: {container_name}, non-zero values/zero values count{len(values)}/{len(non_zero_values)}")
        for value in values:
            csv_write_row(csv_file_path, [value])
        print(f"csv file saved at {csv_file_path}")
    
# Extract the max and average value of each container or pod as csv file
def extract_max_avg_value(file_path, scale):
    if file_is_readable(file_path):
        json_obj = read_json_file(file_path)
    check_meta_data(json_obj)
    result = json_obj['data']["result"]
    csv_file_path=re.sub(r"\.json$", ".csv", file_path)
    if "mem" in file_path.lower() or "etcd_db" in file_path.lower():
        scale = 1073741824
    item = result[0]
    metric_names = list(item['metric'].keys())
    if len(metric_names) < 1:
        sys_exit("no metric names found, please check the json file")
    if "pod" in metric_names:
        metric_type = "pod"
    elif "container" in metric_names:
        metric_type = "container"
    else:
        metric_type = "-".join(metric_names)
    file_name = os.path.basename(file_path)
    file_attr = file_name.split("#")
    namespace = file_attr[0]
    job_type = file_attr[1]
    metric_name = re.sub(r'\..*$', '', file_attr[2])
    csv_write_row(csv_file_path, [metric_type, "Max", "Avg", namespace, job_type, metric_name])
    column_sum = []
    for item in result:
        if len(item['metric']) == 0:
             continue
        values = []
        container_name = '-'.join([ item['metric'][key] for key in metric_names])
        print(item)
        values = [float(value[1])/scale for value in item['values']]
        non_zero_values = [value for value in values if value != float(0)]
        if len(non_zero_values) != len(values) or len(non_zero_values) == 0:
            print(f"found zero value: {container_name}, non-zero values/zero values count{len(values)}/{len(non_zero_values)}")
        max_val = 0.0 if len(non_zero_values) == 0 else max(non_zero_values)
        mean_val = 0.0 if len(non_zero_values) == 0 else sum(non_zero_values)/float(len(non_zero_values))
        row = [container_name, max_val, mean_val]
        csv_write_row(csv_file_path, row)
        print(f"csv file saved at {csv_file_path}")

def is_int(num):
    try:
        int(num)
        return True
    except ValueError:
        print("Error: expecting the time stamp to be an integer")
        return False

def check_time_stamp(stamp):
    if len(stamp) != 4:
        sys_exit(f"found {len(stamp)} parameters in a time stamp, expecting 4 parts in stamp={stamp}, <namespace> <entry type> <start time> <end time>")
    # Assume unix time stamp is either 10 (seconds) or 13 (milliseconds) digits
    if not is_int(stamp[2]) or not is_int(stamp[3]) or len(stamp[2]) not in [10, 13] or len(stamp[3]) not in [10, 13]:
        sys_exit("invalid start and end time stamp, expecting a unix format. eg.1630596238")
    if (int(stamp[3]) - int(stamp[2])) < 0:
        sys_exit("end time stamp is before start time stamp")
    duration = int(stamp[3]) - int(stamp[2])
    print(f"Time stamp durtaion: {stamp[1]} - {duration} seconds")

def read_time_stamps(file_path):
    time_stamp = {}
    if file_is_readable(file_path):
        with open(file_path, 'r') as file_obj:
            for line in file_obj:
                if line.strip().startswith("#"): continue
                if line.strip():
                    entry = line.strip().split()
                    check_time_stamp(entry)
                time_stamp["#".join(entry[:2])]="_".join(entry[2:])
    return time_stamp

def read_yaml(file_path):
    if file_is_readable(file_path):
         with open(file_path, 'r') as file:
            return yaml.safe_load(file)

def parse_metric_profile(file_path):
    if file_is_readable(file_path):
        return read_yaml(file_path)

# time_stamp is a dict uses namespace_entry_type as the key, start_end timestamp as the value
def extract_prom_json_data(metric_profile, time_stamp):
    for entry in time_stamp.keys():
        start = time_stamp[entry].split("_")[0]
        end = time_stamp[entry].split("_")[1]
        # chop everything after and include #
        namespace = re.sub(r'\#.*$','', entry)
        for metric_name in metric_profile.keys():
            QUERY_EXPRESSION=re.sub("placeholder", namespace, metric_profile[metric_name])
            print(f"executing query: {QUERY_EXPRESSION}")
            urlencode_cmd = ["urlencode", QUERY_EXPRESSION]
            encoded_result = subprocess.run(urlencode_cmd, capture_output=True, text="True")
            encoded_expr = encoded_result.stdout.replace('\n', '').replace('\r', '')
            curl_cmd = (
            f"oc exec -n openshift-monitoring -c prometheus prometheus-k8s-1 -- "
            f"curl -s 'http://localhost:9090/api/v1/query_range?"
            f"query={encoded_expr}&start={start}&end={end}&step=5s' | "
            f"jq > {entry}#{metric_name}.json"
            )
            query_result = subprocess.run(curl_cmd, capture_output=True, shell=True)
            print(f"extracted {entry}#{metric_name}.json, status code={query_result.returncode}, stdout={query_result.stdout}, stderr={query_result.stderr}")

def sum_csv_column(csv_file_path):
    max_c = 0
    avg_c = 0
    # matches non / or \ (windows) characters followed by .csv at the end, replace it with column_sum.csv
    # keep the path if it is supplied
    column_sum_path = re.sub(r'[^/\\]*\.csv$', 'column_sum.csv', csv_file_path)
    with open(csv_file_path, 'r') as file:
        csv_obj = csv.reader(file)
        for index, row in enumerate(csv_obj):
            if index == 0:
                csv_write_row(column_sum_path, row)
            else:
                max_c += float(row[1])
                avg_c += float(row[2])
        print(f"total max: {max_c}, avg: {avg_c}")
        csv_write_row(column_sum_path, [max_c, avg_c])
    print(f"saving the column of sum at {column_sum_path}")

# structure the list of csv file in the format of {namespace: {metric_name: [iteration_number]}}
def group_csv_file_paths(csv_files):
    res = {}
    for file_path in csv_files:
        file_attr = re.search(r'[^/]+$', file_path)[0].split("#")
        namespace = file_attr[0]
        iter_num = int(file_attr[1].replace("-iterations", ""))
        metric_name = file_attr[2].replace(".csv", "")
        if namespace not in res.keys():
            res[namespace] = { metric_name: [iter_num]}
        else:
            if metric_name not in res[namespace].keys():
                res[namespace][metric_name] = [iter_num]
            else:
                res[namespace][metric_name].append(iter_num)
                res[namespace][metric_name].sort()
    return res

# aggregate csv data from multiple namespaces and queries
# save the final output as "total_" + namespace + "_" +  metric_name + ".csv"
def aggregate_multi_ns_csv(csv_dir):
    csv_files = read_csv_files(csv_dir)
    if len(csv_files) == 0:
            print("No csv file is found")
            return
    group_csv_file_paths(csv_files)
    structured_file_paths = group_csv_file_paths(csv_files)
    for namespace in structured_file_paths.keys():
        for metric_name in structured_file_paths[namespace]:
            usage_type = metric_name.split("_")[0]
            aggregated_csv_path = "total_" + namespace + "_" +  metric_name + ".csv"
            csv_headers = ["pod count", "max-"+ usage_type, "avg-" + usage_type]
            csv_write_row(aggregated_csv_path, csv_headers)
            all_rows = []
            for iter_num in structured_file_paths[namespace][metric_name]:
                file_path = f"{namespace}#{iter_num}-iterations#{metric_name}.csv"
                print(file_path)
                added_col =[]
                max_c = 0
                avg_c = 0
                csv_obj = read_csv_file(file_path)
                for n, row in enumerate(csv_obj):
                    if n == 0:
                        print(f"adding the column values, headers:{row}")
                        added_col.append(iter_num * 10)
                    else:
                        max_c += float(row[1])
                        avg_c += float(row[2])
                print(f"total max: {max_c}, avg: {avg_c}")
                added_col.append(max_c)
                added_col.append(avg_c)
                all_rows.append(added_col)
            for row in all_rows:
                csv_write_row(aggregated_csv_path, row)

def sum_csv_column_by_dir(csv_dir):
        csv_files = read_csv_files(csv_dir)
        # validate_csv_file_group(csv_files)
        if len(csv_files) == 0:
            print("No csv file is found")
            return
        sys_exit("debugging")
        metric_name = re.search(r'[^/]+$', list(csv_files)[0])[0].split("#")[2]
        column_sum_path = "total_" + metric_name
        usage_type = metric_name.split("_")[0]
        column_sum_headers = ["pod count", "max-"+ usage_type, "avg-" + usage_type]
        csv_write_row(column_sum_path, column_sum_headers)
        all_rows = []
        for file_path in csv_files:
            print(file_path, type(file_path))
            added_col =[]
            max_c = 0
            avg_c = 0
            csv_obj = read_csv_file(file_path)
            for index, row in enumerate(csv_obj):
                if index == 0:
                    print(f"adding the column values, headers:{row}")
                    added_col.append(int(row[4].split("-")[0]) * 10)
                else:
                    max_c += float(row[1])
                    avg_c += float(row[2])
            print(f"total max: {max_c}, avg: {avg_c}")
            added_col.append(max_c)
            added_col.append(avg_c)
            all_rows.append(added_col)
        all_rows.sort(key=lambda row: row[0])
        for row in all_rows:
            csv_write_row(column_sum_path, row)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="command line options for promethus data processing.")
    parser.add_argument('-s', '--scale', type=int, default=1, help="the scale factor number to be divided")
    parser.add_argument('-j', '--file', type=str, help="json file path, extract value and save it as csv file")
    parser.add_argument('-t', '--timestamp', type=str, help="timestamp file path")
    parser.add_argument('-p', '--profile', type=str, help="promethus metric profile path")
    parser.add_argument('-c', '--csv', type=str, help="add csv column value")
    parser.add_argument('-d', '--dir', type=str, help="aggerate multiple csv files from one or more namespaces")

    args = parser.parse_args()
    if bool(args.timestamp) != bool(args.profile):
        parser.error("both --timestamp and --profile file path must be passed together")

    if args.file:
        json_to_csv(args.file)
    elif args.profile or args.timestamp:
        time_stamp = read_time_stamps(args.timestamp)
        metric_profile = read_yaml(args.profile)
        extract_prom_json_data(metric_profile, time_stamp)
    elif args.csv:
        sum_csv_column(args.csv)
    elif args.dir:
        aggregate_multi_ns_csv(args.dir)
