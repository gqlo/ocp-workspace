{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4682f7a2-d057-4ecc-9e0c-92895147f9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing query: baseline_worker_cpu_stdev\n",
      "extracted baseline_worker_cpu_stdev.json, status code=0, stdout=b'', stderr=b''\n",
      "Promethus query status returned success\n",
      "total of 1 entries of data found, 1 entires without metric name, resultType: matrix\n",
      "csv file saved at /home/guoqingli/work/ocp-workspace/prom_extract/baseline_worker_cpu_stdev.csv\n",
      "executing query: baseline_cpu_utilization_across_workers\n",
      "extracted baseline_cpu_utilization_across_workers.json, status code=0, stdout=b'', stderr=b''\n",
      "Promethus query status returned success\n",
      "total of 1 entries of data found, 1 entires without metric name, resultType: matrix\n",
      "csv file saved at /home/guoqingli/work/ocp-workspace/prom_extract/baseline_cpu_utilization_across_workers.csv\n",
      "executing query: baseline_cpu_utilization_by_worker\n",
      "extracted baseline_cpu_utilization_by_worker.json, status code=0, stdout=b'', stderr=b''\n",
      "Promethus query status returned success\n",
      "total of 12 entries of data found, 0 entires without metric name, resultType: matrix\n",
      "csv file saved at /home/guoqingli/work/ocp-workspace/prom_extract/baseline_cpu_utilization_by_worker.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Python scripts process promethus JSON raw metrics\n",
    "\n",
    "def sys_exit(str):\n",
    "    print(f\"{str}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def is_dir(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        print(f\"The directory '{dir_path}' does not exits\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def unix_time(date_str):\n",
    "    \"\"\"\n",
    "    Convert a date string in format \"YYYY-MM-DD HH:MM:SS\" to Unix timestamp in UTC\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): Date string in format \"YYYY-MM-DD HH:MM:SS\"\n",
    "        \n",
    "    Returns:\n",
    "        int: Unix timestamp in seconds\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(datetime.fromisoformat(date_str).replace(tzinfo=timezone.utc).timestamp())\n",
    "    except ValueError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def file_is_readable(file_path):\n",
    "    if not os.access(file_path, os.R_OK):\n",
    "        print(f\"The file at '{file_path}' is not readable\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def obj_exist(obj):\n",
    "    return obj is not None\n",
    "\n",
    "def check_meta_data(json_obj):\n",
    "    status = json_obj.get(\"status\")\n",
    "    if obj_exist(status) and status == \"success\":\n",
    "        print(\"Promethus query status returned success\")\n",
    "    else:\n",
    "        sys_exit(\"Promethus query status returned non-success or status value is not present\")\n",
    "    data = json_obj.get(\"data\")\n",
    "    res_type = data.get(\"resultType\")\n",
    "    res = data.get(\"result\")\n",
    "    empty_metric = 0\n",
    "    if obj_exist(data) and obj_exist(res_type) and obj_exist(res) and len(res) >= 1:\n",
    "        for item in res:\n",
    "            if len(item['metric']) == 0:\n",
    "                empty_metric += 1\n",
    "        print(f\"total of {len(res)} entries of data found, {empty_metric} entires without metric name, resultType: {res_type}\")\n",
    "    else:\n",
    "        sys_exit(\"No data found, please check your json file\")\n",
    "\n",
    "# read the path of json files with given directory\n",
    "def read_json_files(dir_path):\n",
    "    if is_dir(dir_path):\n",
    "        return glob.glob(f\"{dir_path}/*.json\")\n",
    "\n",
    "def read_csv_files(dir_path):\n",
    "    if bool(re.match(r\"^~\", dir_path)):\n",
    "        print(f\"~ symbol found\")\n",
    "        dir_path = os.path.expanduser(dir_path)\n",
    "    if is_dir(dir_path):\n",
    "        print(f\"globing path: {dir_path}\")\n",
    "        glob_obj = glob.glob(f\"{dir_path}/*.csv\")\n",
    "        if len(glob_obj) < 1:\n",
    "            sys_exit(f\"no csv files found at {dir_path}\")\n",
    "        else:\n",
    "            print(f\"{len(glob_obj)} csv files found in total\")\n",
    "            return glob.glob(f\"{dir_path}/*.csv\")\n",
    "    else:\n",
    "        sys_exit(f\"{dir_path} is not recognized as a directory\")\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: file '{file_path}' does not exists\")\n",
    "    except IOError:\n",
    "        print(f\"Error: could not open file '{file_path}'\")\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    try:\n",
    "        return csv.reader(open(file_path, \"r\"))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: file '{file_path}' does not exists\")\n",
    "    except IOError:\n",
    "        print(f\"Error: could not open file '{file_path}'\")\n",
    "\n",
    "# row expects an array\n",
    "def csv_write_row(path, row):\n",
    "    try:\n",
    "        with open(path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: '{e}' occured while writing data into '{path}'\")\n",
    "\n",
    "\n",
    "# structure a raw json object \n",
    "def process_raw_json_obj(json_obj):\n",
    "    res = {}\n",
    "    result = json_obj['data'][\"result\"]\n",
    "    for item in result:\n",
    "        res[list(item['metric'].keys())[0]] = [float(value[1]) for value in item['values']]\n",
    "    return res\n",
    "\n",
    "def json_to_csv(file_path):\n",
    "    if file_is_readable(file_path):\n",
    "        json_obj = read_json_file(file_path)\n",
    "    check_meta_data(json_obj)\n",
    "    result = json_obj['data'][\"result\"]\n",
    "    csv_file_path=re.sub(r\"\\.json$\", \".csv\", file_path)\n",
    "    filename = re.search(r'[^/\\\\]+(?=\\.[^.]+$)', file_path).group(0)\n",
    "    data_points = {}\n",
    "    for item in result:\n",
    "        if len(item['metric']) == 0:\n",
    "            header = filename\n",
    "        else:\n",
    "            header = \"\".join([f\"{key}_{value}\" for key, value in item['metric'].items()])\n",
    "    \n",
    "        values = []\n",
    "        values = [float(value[1]) for value in item['values']]\n",
    "        non_zero_values = [value for value in values if value != float(0)]\n",
    "        if len(non_zero_values) != len(values) or len(non_zero_values) == 0:\n",
    "            print(f\"found zero value: {header}, non-zero values/zero values count{len(values)}/{len(non_zero_values)}\")\n",
    "        data_points[header] = values\n",
    "        \n",
    "    headers = list(data_points.keys())\n",
    "    csv_write_row(csv_file_path, headers)\n",
    "    max_length = max(len(values) for values in data_points.values())\n",
    "    for i in range(max_length):\n",
    "        row = []\n",
    "        for header in headers:\n",
    "            values = data_points[header]\n",
    "            row.append(values[i] if i < len(values) else \"\")\n",
    "        csv_write_row(csv_file_path, row)\n",
    "    print(f\"csv file saved at {csv_file_path}\")\n",
    "    \n",
    "def is_int(num):\n",
    "    try:\n",
    "        int(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        print(\"Error: expecting the time stamp to be an integer\")\n",
    "        return False\n",
    "\n",
    "def validate_metric_profile(metric_profile):\n",
    "    # Check if metric_profile has the required keys\n",
    "    if 'metrics' not in metric_profile or 'global_config' not in metric_profile:\n",
    "        print(\"Error: metric_profile is missing required 'metrics' or 'global_config' sections\")\n",
    "        return False\n",
    "    # Validate each metric\n",
    "    for i, metric in enumerate(metric_profile['metrics']):\n",
    "        # Check that metric is a dictionary\n",
    "        if not isinstance(metric, dict):\n",
    "            print(f\"Error: metric at index {i} is not a dictionary\")\n",
    "            return False\n",
    "        # Check for missing parameters\n",
    "        if metric.get('start') is None and metric_profile['global_config'].get('start') is None:\n",
    "            print(f\"Error: metric '{metric.get('name', f'at index {i}')}' is missing start timestamp\")\n",
    "            return False\n",
    "        if metric.get('end') is None and metric_profile['global_config'].get('start') is None:\n",
    "            print(f\"Error: metric '{metric.get('name', f'at index {i}')}' is missing end timestamp\")\n",
    "            return False\n",
    "        if metric.get('step') is None and metric_profile['global_config'].get('step') is None:\n",
    "            print(f\"Error: metric '{metric.get('name', f'at index {i}')}' is missing step interval\")\n",
    "            return False\n",
    "        # Validate that query exists\n",
    "        if 'query' not in metric or not metric['query']:\n",
    "            print(f\"Error: metric '{metric.get('name', f'at index {i}')}' is missing query expression\")\n",
    "            return False \n",
    "    return True\n",
    "\n",
    "def curl_promethus_endpoint(query_name, start, end, step, query_expression):\n",
    "    print(f\"executing query: {query_name}\")\n",
    "    urlencode_cmd = [\"urlencode\", query_expression]\n",
    "    encoded_url = subprocess.run(urlencode_cmd, capture_output=True, text=\"True\")\n",
    "    encoded_expr = encoded_url.stdout.replace('\\n', '').replace('\\r', '')\n",
    "    curl_cmd = (\n",
    "            f\"oc exec -n openshift-monitoring -c prometheus prometheus-k8s-1 -- \"\n",
    "            f\"curl -s 'http://localhost:9090/api/v1/query_range?\"\n",
    "            f\"query={encoded_expr}&start={start}&end={end}&step={step}' | \"\n",
    "            f\"jq > {query_name}.json\"\n",
    "    )\n",
    "    query_result = subprocess.run(curl_cmd, capture_output=True, shell=True)\n",
    "    if query_result.returncode != 0:\n",
    "        print(f\"Error executing query: {query_result.stderr}\")\n",
    "        return False\n",
    "    print(f\"extracted {query_name}.json, status code={query_result.returncode}, stdout={query_result.stdout}, stderr={query_result.stderr}\")\n",
    "    return f\"{query_name}.json\"\n",
    "\n",
    "def extract_prom_json_data(metric_file_path):\n",
    "     metric_profile = read_json_file(metric_file_path)\n",
    "     validate_metric_profile(metric_profile)\n",
    "     for metric in metric_profile['metrics']:\n",
    "         query_name = metric['name']\n",
    "         query_expression = metric['query']\n",
    "         start = unix_time(metric.get('start') if metric.get('start') is not None else metric_profile['global_config'].get('start'))\n",
    "         end = unix_time(metric.get('end') if metric.get('end') is not None else metric_profile['global_config'].get('end'))\n",
    "         step = metric.get('step') if metric.get('step') is not None else metric_profile['global_config'].get('step')\n",
    "         json_file_name = curl_promethus_endpoint(query_name, start, end, step, query_expression)\n",
    "         file_path = os.path.dirname(metric_file_path) + '/'\n",
    "         json_to_csv(file_path + json_file_name)\n",
    "     \n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"command line options for promethus data processing.\")\n",
    "#     parser.add_argument('-p', '--profile', type=str, help=\"promethus metric profile path\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     if args.profile:\n",
    "#         extract_prom_json_data(metric_profile)\n",
    "\n",
    "extract_prom_json_data(\"/home/guoqingli/work/ocp-workspace/prom_extract/desche_cpu_utilization_profile\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee75b4-f750-4736-b7b4-ca0dec0b041b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0753-3d44-41f3-9833-b787d72bd80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
