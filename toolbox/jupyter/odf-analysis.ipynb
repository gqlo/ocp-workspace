{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd35412-7f18-4eee-928a-23878b9b54af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook, curdoc, output_file, save\n",
    "from bokeh.models import Legend\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource, Select, CustomJS\n",
    "\n",
    "\n",
    "def read_json_file(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}, filename: {file}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e},filename: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def get_max_data_point(json_obj, interval):\n",
    "    raw_stats = json_obj['data']['result'][0]['values']\n",
    "    max_data_points = []\n",
    "    for i in range(0, len(raw_stats), interval):\n",
    "         slices = [float(v[1]) for v in raw_stats[i:i+interval]]\n",
    "         max_data_points.append(math.floor(max(slices)))\n",
    "    return max_data_points\n",
    "\n",
    "def get_latency_stat(json_obj, workload, percentile):\n",
    "    if percentile == \"mean\":\n",
    "        lat = json_obj['jobs'][0][workload]['clat_ns']['mean']\n",
    "    else:\n",
    "        lat = json_obj['jobs'][0][workload]['clat_ns']['percentile'][percentile]\n",
    "    return lat\n",
    "\n",
    "def get_workload(json_obj):\n",
    "    workload = json_obj['jobs'][0][\"job options\"]['rw']\n",
    "    if \"read\" in workload:\n",
    "        return \"read\"\n",
    "    elif \"write\" in workload:\n",
    "        return \"write\"\n",
    "    else:\n",
    "        print(\"unknown workload, exiting the program\")\n",
    "        \n",
    "def get_iops(json_obj):\n",
    "    workload = get_workload(json_obj)\n",
    "    return json_obj['jobs'][0][workload]['iops']\n",
    "\n",
    "def get_avg_clat(json_obj):\n",
    "    workload = get_workload(json_obj)\n",
    "    return json_obj['jobs'][0][workload]['clat_ns']['mean']\n",
    "\n",
    "def get_avg_slat(json_obj):\n",
    "    workload = get_workload(json_obj)\n",
    "    return json_obj['jobs'][0][workload]['slat_ns']['mean']\n",
    "\n",
    "def get_bandwidth(json_obj):\n",
    "    workload = get_workload(json_obj)\n",
    "    return json_obj['jobs'][0][workload]['bw']\n",
    "\n",
    "def get_iodepth(json_obj):\n",
    "    return json_obj['jobs'][0][\"job options\"]['iodepth']\n",
    "\n",
    "def get_block_size(json_obj):\n",
    "    return json_obj['jobs'][0][\"job options\"]['bs']\n",
    "\n",
    "def get_numjobs(json_obj):\n",
    "    return json_obj['jobs'][0][\"job options\"]['numjobs']\n",
    "\n",
    "def get_fio_stats(json_obj):\n",
    "    workload = get_workload(json_obj)\n",
    "    iops = round(get_iops(json_obj))\n",
    "    iodepth = get_iodepth(json_obj)\n",
    "    numjobs = get_numjobs(json_obj)\n",
    "    block_size = get_block_size(json_obj)\n",
    "    avg_slat = round(get_avg_slat(json_obj) / 1000, 2)\n",
    "    avg_clat = round(get_avg_clat(json_obj) / 1000, 2)\n",
    "    bw = round(get_bandwidth(json_obj) / 1024, 2)  # MB unit\n",
    "    print(workload, numjobs, block_size, iodepth, avg_slat, avg_clat, iops, bw)\n",
    "    return [workload, numjobs, block_size, iodepth, avg_slat, avg_clat, iops, bw]\n",
    "    \n",
    "def write_row(csv_path, arr):\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(arr)\n",
    "\n",
    "def create_csv_table(csv_path, data):\n",
    "    for i in range(len(data[blocks[0]])):\n",
    "        tmp_row = []\n",
    "        for blk in blocks:\n",
    "            tmp_row.append(data[blk][i])\n",
    "        write_row(csv_path, tmp_row) \n",
    "        \n",
    "def generate_io_rate_range(min_rate, max_rate):\n",
    "    arr = []\n",
    "    start = min_rate // 30\n",
    "    return [n//52 for n in range(start, max_rate, start)]\n",
    "\n",
    "def generate_file_path(vmid, blk, rate):\n",
    "    return f\"{pg_256_dir}pg_256_2cpu_5g-rhel9-snap-{vmid}-{blk}-randread-depth-16-rate-{rate}.json\"\n",
    "    #return f\"{pg_256_dir}pg_256_io_fixed_randread_no_scrub_2_CPU_5G_RAM_4k_128k-vm-count-52-rhel9-snap-{vmid}-{blk}-randread-depth-16-rate-{rate}.json\"\n",
    "\n",
    "\n",
    "def aggregate_json_stat(workload, percentile, io_rate):\n",
    "    d = {}\n",
    "    vm = [k for k in range(1,53)] \n",
    "    vm.remove(44)\n",
    "    for blk in blocks:\n",
    "        for rate in io_rate:\n",
    "            tmp_arr = []\n",
    "            for vmid in vm:\n",
    "                file = generate_file_path(vmid, blk, rate)\n",
    "                tmp = get_latency_stat(read_json_file(file), workload, percentile)\n",
    "                tmp_arr.append(tmp)\n",
    "            if blk not in d.keys():\n",
    "                d[blk] = {'x':[], 'y': []}\n",
    "            d[blk]['y'].append(round(sum(tmp_arr)/(10**5)/len(vm),2))\n",
    "            d[blk]['x'].append(rate*52//1000)\n",
    "    return {f\"fio_{key}\": value for key, value in d.items()}\n",
    "\n",
    "def create_interactive_plot(datasets):\n",
    "    output_notebook()\n",
    "    blk_size = list(datasets.keys())\n",
    "    print(blk_size)\n",
    "   \n",
    "    # Create the ColumnDataSource\n",
    "    source = ColumnDataSource(data=datasets[blk_size[0]])\n",
    "\n",
    "    # Create the figure\n",
    "    p = figure(title=\"randread - example plot\", x_axis_label=\"IOPS (K)\", y_axis_label=\"ms\")\n",
    "    p.line('x', 'y',source=source)\n",
    "\n",
    "    # Create the CustomJS callback arguments dynamically\n",
    "    args = {'source': source}\n",
    "  \n",
    "    for blk in datasets:\n",
    "        args[blk] = datasets[blk]\n",
    "    # Generate the JavaScript code to handle the datasets\n",
    "    js_code = \"\"\"\n",
    "        var data_sets = {\"\"\"\n",
    "    for key in datasets:\n",
    "        js_code += f\"'{key}': {key}, \"\n",
    "    js_code = js_code[:-2]  # Remove the last comma and space\n",
    "    js_code += \"\"\"};\n",
    "        source.data = data_sets[cb_obj.value];\n",
    "        source.change.emit();\n",
    "    \"\"\"\n",
    " \n",
    "    print(\"js_code=\",js_code)\n",
    "    # Create the CustomJS callback\n",
    "    callback = CustomJS(args=args, code=js_code)\n",
    "\n",
    "    # Create a Select widget with options for all datasets\n",
    "    select = Select(title=\"Select block size\", value=blk_size[0], options=blk_size)\n",
    "    select.js_on_change('value', callback)\n",
    "\n",
    "    # Layout and add to the current document\n",
    "    layout = column(select, p)\n",
    "    curdoc().add_root(layout)\n",
    "    output_file(\"bokeh_plot.html\")\n",
    "    show(layout)\n",
    "\n",
    "\n",
    "# pg_256_dir = \"/home/guoqingli/work/data/cnv-odf/pg_256_default/\"\n",
    "# colors =[\"blue\", \"red\", \"yellow\", \"green\", \"orange\", \"purple\"]\n",
    "# blocks= [\"4k\", \"8k\", \"16k\", \"32k\", \"64k\", \"128k\"]\n",
    "# # blocks= [\"256k\", \"512k\"]\n",
    "# # blocks= [\"1024k\", \"2048k\"]\n",
    "\n",
    "# # blk_size = {\"256k\": \"indigo\",\"512k\":\"darkslateblue\"}\n",
    "# # block 4k to 128k\n",
    "# rw_rate_4k_to_128k = [32,65,97,130,162,195,227,260,292,325,357,390,422,455,487,520,552,585,617,650,682,715,747,780,812,845,877,910]\n",
    "# rw_rate_256k_to_512k = [16,32,49,65,82,98,114,131,147,164,180,197,213,229,246,262,279,295,312,328,344,361,377,394,410,427,443,459]\n",
    "# rw_rate_1024k_to_2048k = generate_io_rate_range(5938, 10312)\n",
    "# rr_rate_4k_to_128k = generate_io_rate_range(188622, 390467)\n",
    "\n",
    "\n",
    "# rr_rate_256_to_512k = generate_io_rate_range(90590, 143879)\n",
    "# agg_stat = aggregate_json_stat(\"read\", \"99.000000\", rr_rate_4k_to_128k)\n",
    "# create_interactive_plot(agg_stat)\n",
    "# sys.exit(1)\n",
    "#table_path=\"/home/guoqingli/work/data/cnv-odf/randwrite_pg_256_2cpu_5g_1024_2048k.csv\"\n",
    "\n",
    "# write headers\n",
    "#write_row(table_path, blocks)\n",
    "#create_csv_table(table_path, agg_stat)\n",
    "\n",
    "#sys.exit(1)\n",
    "# p = figure(title=\"randwrite p99 latency 26 OSDs 2CPU 5G RAM\", x_axis_label='IOPS', y_axis_label='ms', y_axis_type=\"log\")\n",
    "# legend_items = []\n",
    "# for blk in blk_size.keys():\n",
    "#     tmp_p = p.line(d[blk]['x_axis'], d[blk]['y_axis'],line_width=1.5, color=blk_size[blk])\n",
    "#     legend_items.append([blk, [tmp_p]])\n",
    "\n",
    "# legend = Legend(items=legend_items)\n",
    "# p.add_layout(legend, 'right')\n",
    "# show(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfa02c90-9876-40c7-b8df-8e2248fe84cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1 4k 1 4.16 34.54 25574 99.9\n",
      "read 1 4k 2 3.0 48.55 38512 150.44\n",
      "read 1 4k 4 3.03 62.94 60291 235.51\n",
      "read 1 4k 8 2.67 95.98 80799 315.62\n",
      "read 1 4k 16 2.48 150.44 104381 407.74\n",
      "read 1 4k 32 2.17 193.63 163137 637.25\n",
      "read 1 4k 64 2.13 250.94 252500 986.33\n",
      "read 1 4k 128 2.05 387.84 327961 1281.1\n",
      "read 1 4k 256 2.04 780.49 326975 1277.25\n",
      "read 1 4k 512 2.04 1580.46 323447 1263.46\n",
      "read 1 4k 1024 2.02 3085.97 331563 1295.17\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/guoqingli/work/data/cnv-odf/rhel9-lvm/\"\n",
    "csv_path = path + \"csv/lvm_read.csv\"\n",
    "header = [\"workload\", \"numjobs\", \"block_size\", \"io_depth\", \"slat (us)\", \"clat (us)\",\"iops\", \"bandwidth (MB/s)\"]\n",
    "write_row(csv_path, header)\n",
    "for depth in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    json_obj =  read_json_file(path+ \"lvm-rhel9-lvm-4k-read-depth-\" + str(depth) + \"-numjob-1-rate-50000000.json\")\n",
    "    write_row(csv_path, get_fio_stats(json_obj))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392009e-47ad-43a5-8ccd-685429a574ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace56e5c-45d3-4682-bad4-d48681996844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
